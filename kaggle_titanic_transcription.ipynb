#사용할 라이브러리 임포트
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt   # 시각화 라이브러리 
import seaborn as sns             # 시각화 라이브러리
plt.style.use("fivethirtyeight")  # 'fivethirtyeight'이라는 style 테마를 사용
import warnings                   
warnings.filterwarnings('ignore') # 버젼 차이로 인해 출력되는 에러 문구등을 무시하겠다는 명령어
%matplotlib inline

#캐글의 Api를 받아와서 구글 코랩에서 Kaggle의 데이터셋을 사용하기 위하는 명령어들
!pip install kaggle --upgrade 
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c titanic
#-------------------------------------------------------------
# Kaggle의 EDA To Prediction(DieTanic) 링크 : https://www.kaggle.com/ash316/eda-to-prediction-dietanic 
# 필사를 하면서 새롭게 알게되는 내용

data = pd.read_csv("train.csv") #캐글의 타이타닉 데이터 불러오기


data['Initial'] = 0 #'Initial' Series를 0으로 만들기

#'Name' Series에서 영어로 구성되어있으며 마지막에 .(dot)으로 끝나는 단어를 찾아서 'Initial' Series에 할당
for i in data:
  data['Initial'] = data['Name'].str.extract('([A-Za-z]+)\.') 

plt.close() #현재 창에서 figure를 닫을 때 사용하는 함수, 매개변수로 None이면 현재 figure

pd.crosstab(index=data.SibSp, columns=data.Pclass).style.background_gradient(cmap='summer_r') #파라미터로 index와 columns를 받음
pd.crosstab(data.SibSp, data.Pclass).style.background_gradient(cmap='summer_r') #파라미터인 index와 columns 설정을 생략해도 가능

sns.barplot(x=data.Parch, y=data.Survived, ax=axes[0]) #아래의 코드와 같은 코드지만 다르게 작성할 수 있음
sns.barplot('Parch','Survived',data=data, ax=axes[0])

sns.factorplot(x=data.Parch, y=data.Survived,data=data, ax=axes[1]) #아래의 코드와 같은 코드지만 다르게 작성할 수 있음
sns.factorplot(x='Parch', y='Survived',data=data, ax=axes[1])

data.Fare == data['Fare'] # 두 코드는 같은 기능을 함 Series를 표현함

data[data.Pclass==1].Fare # 이 코드는 Pclass 특성에서 1등급인 Fare의 Series를 반환함

sns.heatmap() #데이터들을 색상으로 표현해주는 그래프
data.corr # pandas.DataFrame.corr()이고, corr은 correlation(상관관계)의 줄임말이다

data.loc[data['Age'] <= 16, 'Age_band']=0 # Age 특성에서 16보다 이하인 값을 모두 찾고, 조건에 맞는 Age_band의 특성을 0으로 만들겠다는 의미
data['Age_band'].value_counts() # Age_band Series의 값별 개수 세기
Series.to_frame() # Seires를 DataFrame으로 바꾸는 함수

pd.qcut(data['Fare'],4) #Fare Series 데이터를 4개의 범위로 균등하게 나누어주고 반환함

pd.DataFrame({'CV Mean' : mean, 'Std': std}, index=classifiers) #데이터 프레임을 만드는 함수 매개변수로 딕셔너리 형태와 index를 전달 이때 딕셔너리의 key값이 colums의 이름,value가 각 값이 됨

#predict modeling
#새로 알게된 ML
from sklearn import svs #Support Vector Machine svm모델이 어떻게 작동하는지 모름
from sklearn.naive_bayes import GaussianNB #Naive bayes 마찬가지로 모델이 어떻게 작동하는지 모름
from sklearn import metrics #정확도 측정을 위한 임포트, metrics(척도)
from sklearn.metrics import cunfusion_matrix #정확도를 측정하기 위하여 혼돈행렬을 임포트


lr_model = LogisticRegression()
#분류한 훈련세트를 훈련함
lr_model.fit(train_X, train_Y)
#훈련된 데이터를 기반으로 테스트 데이터를 예측함 --> 어떤 배열이 결과로 나옴
prediction_lr = lr_model.predict(test_X)
print('The prediction of the Logistic Regression : ', prediction_lr)
# metrics 패키지를 사용하여 정확도를 측정
# 예측한 테스트데이터와, 실제로 분류하였던 test_Y 데이터를 비교하여 정확도를 측정함
print('The accuracy of the Logistic Regression : ', metrics.accuracy_score(prediction_lr, test_Y))
#훈련데이터의 점수를 확인함
print('The Train Score of the Logistic Regression : ', lr_model.score(train_X, train_Y))
#테스트데이터의 점수를 확인하는데 위의 정확도와 같은 결과를 냄
print('The Test Score of the Logistic Regression : ', lr_model.score(test_X, test_Y))

#Cross Validataion

new_models_dataframe['CV_Mean'].plot.barh(width=0.8) #Series 데이터의 가로 막대 그래프를 그리는 함수, width(너비) = 0.8로

plt.subplots_adjust(hspace=0.2,wspace=0.2) #서브플롯 사이의 간격을 조절 hspace는 높이, wspace는 너비


#Ensenble
from sklearn.ensemble import VotingClassifier
models = [
          ('KNN',KNeighborsClassifier(n_neighbors=10)),
          ('RBF',svm.SVC(probability=True, kernel='rbf',C=0.5,gamma=0.1)),
          ('SVM',svm.SVC(probability=True, kernel='linear')),
          ('RandomForest',RandomForestClassifier(n_estimators=500,random_state=0)),
          ('LogisticRegression',LogisticRegression(C=0.05)),
          ('DecisionTree',DecisionTreeClassifier(random_state=0)),
          ('NB',GaussianNB())
]
vot =VotingClassifier(estimators = models) # 매개변수인 estimators는 tuple(str,estimator)으로 구성된 리스트를 받는다, voting 매개변수는 타입을 결정하는데 'soft','hard'가 있다




